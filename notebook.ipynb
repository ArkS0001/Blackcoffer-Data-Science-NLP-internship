{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_7916\\2976136187.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was too old on your system - pyarrow 10.0.1 is the current minimum supported version as of this release.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"Input.xlsx\"\n",
    "output_file = \"Output Data.xlsx\"\n",
    "\n",
    "input_df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = {\n",
    "    \"URL_ID\": [],\n",
    "    \"URL\": [],\n",
    "    \"ARTICLE_TITLE\": [],\n",
    "    \"ARTICLE_TEXT\": [],\n",
    "    \"POSITIVE_SCORE\": [],\n",
    "    \"NEGATIVE_SCORE\": [],\n",
    "    \"POLARITY_SCORE\": [],\n",
    "    \"SUBJECTIVITY_SCORE\": [],\n",
    "    \"AVG_SENTENCE_LENGTH\": [],\n",
    "    \"PERCENTAGE_OF_COMPLEX_WORDS\": [],\n",
    "    \"FOG_INDEX\": [],\n",
    "    \"AVG_NUMBER_OF_WORDS_PER_SENTENCE\": [],\n",
    "    \"COMPLEX_WORD_COUNT\": [],\n",
    "    \"WORD_COUNT\": [],\n",
    "    \"SYLLABLE_PER_WORD\": [],\n",
    "    \"PERSONAL_PRONOUNS\": [],\n",
    "    \"AVG_WORD_LENGTH\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find article title\n",
    "    article_title = soup.find('title').text.strip()\n",
    "    \n",
    "    # Find article text\n",
    "    article_text = \"\"\n",
    "    for paragraph in soup.find_all('p'):\n",
    "        article_text += paragraph.text.strip() + \"\\n\"\n",
    "    \n",
    "    return article_title, article_text\n",
    "\n",
    "# Function to calculate syllables in a word\n",
    "def syllable_count(word):\n",
    "    word = word.lower()\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Function to compute text analysis\n",
    "def analyze_text(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    positive_score = sum(1 for word in blob.words if TextBlob(word).sentiment.polarity > 0)\n",
    "    negative_score = sum(1 for word in blob.words if TextBlob(word).sentiment.polarity < 0)\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    avg_sentence_length = sum(len(sent.split()) for sent in sentences) / len(sentences)\n",
    "    complex_words = [word for word in words if syllable_count(word) > 2 and word.lower() not in stop_words]\n",
    "    percentage_complex_words = (len(complex_words) / len(words)) * 100 if len(words) > 0 else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    avg_words_per_sentence = len(words) / len(sentences)\n",
    "    complex_word_count = len(complex_words)\n",
    "    word_count = len(words)\n",
    "    syllables = sum(syllable_count(word) for word in words)\n",
    "    syllable_per_word = syllables / word_count if word_count > 0 else 0\n",
    "    personal_pronouns = sum(1 for word in words if word.lower() in ['i', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves'])\n",
    "    avg_word_length = sum(len(word) for word in words) / len(words) if len(words) > 0 else 0\n",
    "    \n",
    "    return (positive_score, negative_score, polarity_score, subjectivity_score, \n",
    "            avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, \n",
    "            complex_word_count, word_count, syllable_per_word, personal_pronouns, avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in input_df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    # Extract text from URL\n",
    "    article_title, article_text = extract_text_from_url(url)\n",
    "    \n",
    "    # Perform text analysis\n",
    "    (positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, \n",
    "     percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, \n",
    "     syllable_per_word, personal_pronouns, avg_word_length) = analyze_text(article_text)\n",
    "    \n",
    "    # Store data in output dictionary\n",
    "    output_data[\"URL_ID\"].append(url_id)\n",
    "    output_data[\"URL\"].append(url)\n",
    "    output_data[\"ARTICLE_TITLE\"].append(article_title)\n",
    "    output_data[\"ARTICLE_TEXT\"].append(article_text)\n",
    "    output_data[\"POSITIVE_SCORE\"].append(positive_score)\n",
    "    output_data[\"NEGATIVE_SCORE\"].append(negative_score)\n",
    "    output_data[\"POLARITY_SCORE\"].append(polarity_score)\n",
    "    output_data[\"SUBJECTIVITY_SCORE\"].append(subjectivity_score)\n",
    "    output_data[\"AVG_SENTENCE_LENGTH\"].append(avg_sentence_length)\n",
    "    output_data[\"PERCENTAGE_OF_COMPLEX_WORDS\"].append(percentage_complex_words)\n",
    "    output_data[\"FOG_INDEX\"].append(fog_index)\n",
    "    output_data[\"AVG_NUMBER_OF_WORDS_PER_SENTENCE\"].append(avg_words_per_sentence)\n",
    "    output_data[\"COMPLEX_WORD_COUNT\"].append(complex_word_count)\n",
    "    output_data[\"WORD_COUNT\"].append(word_count)\n",
    "    output_data[\"SYLLABLE_PER_WORD\"].append(syllable_per_word)\n",
    "    output_data[\"PERSONAL_PRONOUNS\"].append(personal_pronouns)\n",
    "    output_data[\"AVG_WORD_LENGTH\"].append(avg_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(output_data)\n",
    "output_df.to_excel(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
